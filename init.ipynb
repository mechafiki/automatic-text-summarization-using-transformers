{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                                                    # linear algebra\n",
    "import torch                                                                                          # pytorch\n",
    "import transformers                                                                                   # huggingface transformers\n",
    "from sklearn.model_selection import train_test_split                                                  # split data\n",
    "from transformers import AutoTokenizer                                                                # tokenizer\n",
    "import pandas as pd                                                                                   # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from datasets import load_dataset                                                                     # load dataset\n",
    "import glob                                                                                           # glob for file path\n",
    "import os                                                                                             # os for file path\n",
    "import re                                                                                             # regex\n",
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments,AutoModelWithLMHead   # T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):                                                              # function to read data from the path\n",
    "    data = []\n",
    "    for topic in os.listdir(path):\n",
    "        for file in os.listdir(path + \"/\" + topic):\n",
    "            with open(path + \"/\" + topic + \"/\" + file) as f:\n",
    "                data.append(f.read())\n",
    "    return data\n",
    "\n",
    "original_text = read_data(\"files/BBC News Summary/Summaries\")                      # read the original text\n",
    "summary_text = read_data(\"files/BBC News Summary/News Articles\")                   # read the summary text\n",
    "\n",
    "df = pd.DataFrame({'original':original_text,'summary':summary_text})               # create a dataframe\n",
    "\n",
    "df.to_csv('files/summary.csv', index=False)                                        # export the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-49933766bd4604e6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/moham/.cache/huggingface/datasets/csv/default-49933766bd4604e6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 285.17it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]C:\\Users\\moham\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\download\\streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n",
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/moham/.cache/huggingface/datasets/csv/default-49933766bd4604e6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='files/summary.csv', split='train')       # load the dataset\n",
    "dataset = dataset.train_test_split(test_size=0.1)                                  # split to train and test\n",
    "train_dataset = dataset['train']                                                   # train dataset\n",
    "val_dataset = dataset['test']                                                      # test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.24s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.03ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')                                 # load the tokenizer\n",
    "\n",
    "def tokenize(batch):                                                                 # function to tokenize the data\n",
    "    tokenized_input = tokenizer(batch['original'],                                   # tokenize the input and label\n",
    "                                padding='max_length',                                # pad the input and label\n",
    "                                truncation=True,                                     # truncate the input and label\n",
    "                                max_length=512)                                      # max length of the input and label\n",
    "    tokenized_label = tokenizer(batch['summary'], \n",
    "                                padding='max_length', \n",
    "                                truncation=True, \n",
    "                                max_length=159)\n",
    "\n",
    "    tokenized_input['labels'] = tokenized_label['input_ids']                         # add the label to the input\n",
    "\n",
    "    return tokenized_input                                                           # return the tokenized input\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=512)            # tokenize the train dataset\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))   # tokenize the test dataset\n",
    "\n",
    "train_dataset.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels']) # set the format of the train dataset\n",
    "val_dataset.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])   # set the format of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')                         # load the model\n",
    "\n",
    "output_dir = './output_dir'                                                           # output directory\n",
    "\n",
    "training_args = TrainingArguments(                                                    # define the training arguments\n",
    "    output_dir=output_dir,                                                            # output directory\n",
    "    num_train_epochs=1,                                                               # number of training epochs, we set it to 1 because we are using wandb to track the training process\n",
    "                                                                                      # and we can stop the training whenever we want, plus each epoch takes a lot of time\n",
    "    per_device_train_batch_size=8,                                                    # batch size, we set it to 8 because we are using a GPU with 8GB of vRAM\n",
    "    per_device_eval_batch_size=8,                                                     # batch size, we set it to 8 because we are using a GPU with 8GB of vRAM\n",
    "    eval_accumulation_steps=1,                                                        # number of eval steps to keep in GPU (the higher, the mor vRAM used)\n",
    "    prediction_loss_only=True,                                                        # if I need co compute only loss and not other metrics, setting this to true will use less RAM\n",
    "    learning_rate=0.1,                                                                # learning rate (the higher, the faster the model will learn, \n",
    "                                                                                      # but it can also lead to divergence)\n",
    "    evaluation_strategy='steps',                                                      # run evaluation every eval_steps\n",
    "    save_steps=1000,                                                                  # how often to save a checkpoint\n",
    "    save_total_limit=1,                                                               # number of maximum checkpoints to save\n",
    "    remove_unused_columns=True,                                                       # removes useless columns from the dataset\n",
    "    run_name='run_name',                                                              # wandb run name\n",
    "    logging_steps=1000,                                                               # how often to log loss to wandb\n",
    "    eval_steps=1000,                                                                  # how often to run evaluation on the val_set\n",
    "    logging_first_step=False,                                                         # whether to log also the very first training step to wandb\n",
    "    load_best_model_at_end=True,                                                      # whether to load the best model found at each evaluation.\n",
    "    metric_for_best_model=\"loss\",                                                     # use loss to evaluate best model.\n",
    "    greater_is_better=False                                                           # best model is the one with the lowest loss, not highest.\n",
    ")\n",
    "\n",
    "trainer = Trainer(                                                                    # define the trainer\n",
    "    model=model,                                                                      # model to train\n",
    "    args=training_args,                                                               # training arguments\n",
    "    train_dataset=train_dataset,                                                      # train dataset       \n",
    "    eval_dataset=val_dataset                                                          # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output_dir/model_\n",
      "Configuration saved in ./output_dir/model_\\config.json\n",
      "Model weights saved in ./output_dir/model_\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir + '/model_')                                            # save the model in order to continue training later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./output_dir/model_\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"./output_dir/model_\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file ./output_dir/model_\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"./output_dir/model_\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ./output_dir/model_\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./output_dir/model_.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline('summarization',                                                # define the summarizer\n",
    "                        model=output_dir + '/model_',                                 # model path\n",
    "                        tokenizer=tokenizer,                                          # tokenizer\n",
    "                        framework='pt')                                               # framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style='color:red;'>Notice</p> \n",
    "#### There is a big limitation here, the training is done on a single GPU, so it takes a lot of time to train the model, I tried to train it on a Google Colab Pro, but it was still too slow, so I decided to use wandb to track the training process and stop it whenever I want, then I save the model and use it to generate summaries.\n",
    "\n",
    "#### Even though, I trained the model for 1 epoch, it took me more than 6 hours to train it, so I decided to use a pretrained model, which I will show you in the next section.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating summaries using t5-small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):                                                              # function to generate the summary\n",
    "\n",
    "    model = AutoModelWithLMHead.from_pretrained(\"t5-small\")                              # load the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")                                # load the tokenizer\n",
    "\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text,                                      # encode the input\n",
    "                                return_tensors=\"pt\",                                     # return tensors\n",
    "                                max_length=512,                                          # max length of the input\n",
    "                                truncation=True)                                         # truncate the input\n",
    "\n",
    "    outputs = model.generate(inputs,                                                     # generate the summary\n",
    "                                max_length=250,                                          # max length of the summary (the higher, the more the model will try to generate a longer summary)\n",
    "                                min_length=80,                                          # min length of the summary (the higher, the more the model will try to generate a longer summary)\n",
    "                                length_penalty=2.0,                                      # length penalty (the higher, the more the model will try to generate a longer summary)\n",
    "                                num_beams=4,                                             # number of beams (the higher, the more the model will try to generate a longer summary)\n",
    "                                early_stopping=True)                                     # early stopping (if the model generates a summary that is longer than the max length, it will stop)\n",
    "    return tokenizer.decode(outputs[0])                                                  # return the summary\n",
    "\n",
    "\n",
    "def generate_summary_for_text(text):                                                     # function to generate the summary for a text\n",
    "\n",
    "    # text = re.sub(r'\\d+', '', text)                                                      # remove the numbers (optional because the model is trained on numbers too)\n",
    "    text = re.sub(' +', ' ', text)                                                       # remove the extra spaces\n",
    "    text = re.sub('\\n+', ' ', text)                                                      # remove the extra new lines\n",
    "    text = re.sub('\\t+', ' ', text)                                                      # remove the extra tabs\n",
    "\n",
    "    summary = generate_summary(text)                                                     # generate the summary               \n",
    "    return summary                                                                       # return the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted summary :\n",
      " <pad> a common technical definition of a recession is two successive quarters of negative growth. on an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. japan's economy teetered on the brink of a technical recession in the three months to September, figures show.</s>\n",
      " \n",
      "Original summary :\n",
      " Japan narrowly escapes recession\n",
      "\n",
      "Japan's economy teetered on the brink of a technical recession in the three months to September, figures show.\n",
      "\n",
      "Revised figures indicated growth of just 0.1% - and a similar-sized contraction in the previous quarter. On an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. A common technical definition of a recession is two successive quarters of negative growth.\n",
      "\n",
      "The government was keen to play down the worrying implications of the data. \"I maintain the view that Japan's economy remains in a minor adjustment phase in an upward climb, and we will monitor developments carefully,\" said economy minister Heizo Takenaka. But in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead, observers were less sanguine. \"It's painting a picture of a recovery... much patchier than previously thought,\" said Paul Sheard, economist at Lehman Brothers in Tokyo. Improvements in the job market apparently have yet to feed through to domestic demand, with private consumption up just 0.2% in the third quarter.\n",
      "\n",
      "Predicted summary word count :  56\n",
      "Original summary word count :  184\n"
     ]
    }
   ],
   "source": [
    "summary_ = generate_summary_for_text(df['original'][5])\n",
    "\n",
    "print('Predicted summary :\\n',summary_)\n",
    "print(\" \")\n",
    "print('Original summary :\\n',df['summary'][5])\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "print('Predicted summary word count : ',count_words(summary_))\n",
    "print('Original summary word count : ',count_words(df['summary'][5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a63501da4ff5345f792f8d85fdbe5e308a00ec1c83390f9577f8c8a1adb4af3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
