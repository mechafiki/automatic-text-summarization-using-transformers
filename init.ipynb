{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                                                    # Algèbre linéaire\n",
    "import torch                                                                                          # pytorch\n",
    "import transformers                                                                                   # huggingface transformers\n",
    "from sklearn.model_selection import train_test_split                                                  # division train/test\n",
    "from transformers import AutoTokenizer                                                                # tokenizer\n",
    "import pandas as pd                                                                                   # traitement de données\n",
    "from datasets import load_dataset                                                                     # chargement de dataset\n",
    "import glob                                                                                           # glob pour les fichiers\n",
    "import os                                                                                             # os pour les fichiers\n",
    "import re                                                                                             # regex\n",
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments,AutoModelWithLMHead   # modèle T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):                                                              # fonction pour lire les fichiers\n",
    "    data = []\n",
    "    for topic in os.listdir(path):\n",
    "        for file in os.listdir(path + \"/\" + topic):                               # parcourir les fichiers par topic\n",
    "            with open(path + \"/\" + topic + \"/\" + file) as f:\n",
    "                data.append(f.read())\n",
    "    return data\n",
    "\n",
    "original_text = read_data(\"files/BBC News Summary/Summaries\")                      # lire le texte original\n",
    "summary_text = read_data(\"files/BBC News Summary/News Articles\")                   # lire le texte résumé\n",
    "\n",
    "df = pd.DataFrame({'original':original_text,'summary':summary_text})               # créer un dataframe avec les deux colonnes\n",
    "\n",
    "df.to_csv('files/summary.csv', index=False)                                        # sauvegarder le dataframe en csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-49933766bd4604e6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/moham/.cache/huggingface/datasets/csv/default-49933766bd4604e6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 285.17it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]C:\\Users\\moham\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\download\\streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n",
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/moham/.cache/huggingface/datasets/csv/default-49933766bd4604e6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='files/summary.csv', split='train')       # charger le dataset\n",
    "dataset = dataset.train_test_split(test_size=0.1)                                  # diviser le dataset en train/test\n",
    "train_dataset = dataset['train']                                                   # train dataset\n",
    "val_dataset = dataset['test']                                                      # test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.24s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.03ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')                                 # charger le tokenizer\n",
    "\n",
    "def tokenize(batch):                                                                 # fonction pour tokeniser les données\n",
    "    tokenized_input = tokenizer(batch['original'],                                   # tokenizer le texte original\n",
    "                                padding='max_length',                                # ajouter du padding pour avoir des inputs de même taille\n",
    "                                truncation=True,                                     # tronquer les inputs trop longs\n",
    "                                max_length=512)                                      # taille max de l'input (512 pour T5)\n",
    "    tokenized_label = tokenizer(batch['summary'], \n",
    "                                padding='max_length', \n",
    "                                truncation=True, \n",
    "                                max_length=159)\n",
    "\n",
    "    tokenized_input['labels'] = tokenized_label['input_ids']                         # ajouter les labels aux inputs\n",
    "\n",
    "    return tokenized_input                                                           # retourner les inputs tokenisés\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=512)            # tokenizer le dataset d'entrainement\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))   # tokenizer le dataset de test\n",
    "\n",
    "train_dataset.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels']) # mettre le format du dataset d'entrainement en numpy\n",
    "val_dataset.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])   # mettre le format du dataset de test en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')                         # charger le modèle T5\n",
    "\n",
    "output_dir = './output_dir'                                                           # dossier de sortie\n",
    "\n",
    "training_args = TrainingArguments(                                                    # définition des paramètres d'entrainement\n",
    "    output_dir=output_dir,                                                            # dossier de sortie\n",
    "    num_train_epochs=1,                                                               # nombre d'époques d'entraînement, nous le définissons sur 1 car nous utilisons wandb \n",
    "                                                                                      # pour suivre le processus d'entraînement\n",
    "                                                                                      # et on peut arrêter l'entraînement quand on veut, \n",
    "                                                                                      # en plus chaque époque prend beaucoup de temps\n",
    "    per_device_train_batch_size=8,                                                    # taille du batch d'entraînement, nous le définissons sur 8 car on a un GPU avec 128MB de vRAM\n",
    "    per_device_eval_batch_size=8,                                                     # \n",
    "    eval_accumulation_steps=1,                                                        # nombre d'étapes d'évaluation à conserver dans le GPU (plus il est élevé, plus la vRAM utilisée)\n",
    "    prediction_loss_only=True,                                                        # si on a besoin de co calculer uniquement la perte et pas d'autres mesures, \n",
    "                                                                                      # le régler sur vrai utilisera moins de RAM\n",
    "    learning_rate=0.1,                                                                # taux d'apprentissage (plus il est élevé, plus l'entraînement est rapide, \n",
    "                                                                                      # mais il peut conduire à la divergence)\n",
    "    evaluation_strategy='steps',                                                      # comment évaluer le modèle, ici on évalue le modèle à chaque 1000 étapes\n",
    "    save_steps=1000,                                                                  # sauvegarder le modèle à chaque 1000 étapes\n",
    "    save_total_limit=1,                                                               # nombre maximal de modèles à sauvegarder\n",
    "    remove_unused_columns=True,                                                       # supprimer les colonnes non utilisées du dataset\n",
    "    run_name='run_name',                                                              # nom de l'entraînement (pour wandb)\n",
    "    logging_steps=1000,                                                               # enregistrer les logs à chaque 1000 étapes\n",
    "    eval_steps=1000,                                                                  # évaluer le modèle à chaque 1000 étapes\n",
    "    logging_first_step=False,                                                         # ne pas enregistrer les logs pour la première étape\n",
    "    load_best_model_at_end=True,                                                      # charger le meilleur modèle à la fin de l'entraînement\n",
    "    metric_for_best_model=\"loss\",                                                     # métrique à utiliser pour déterminer le meilleur modèle (ici la perte)\n",
    "    greater_is_better=False                                                           # si la métrique est meilleure quand elle est plus grande ou plus petite\n",
    ")\n",
    "\n",
    "trainer = Trainer(                                                                    # définition du trainer\n",
    "    model=model,                                                                      # modèle à entrainer\n",
    "    args=training_args,                                                               # paramètres d'entrainement\n",
    "    train_dataset=train_dataset,                                                      # dataset d'entrainement    \n",
    "    eval_dataset=val_dataset                                                          # dataset d'évaluation\n",
    ")\n",
    "\n",
    "trainer.train()                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output_dir/model_\n",
      "Configuration saved in ./output_dir/model_\\config.json\n",
      "Model weights saved in ./output_dir/model_\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir + '/model_')                                            # sauvegarder le modèle pour le réutiliser utérieurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./output_dir/model_\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"./output_dir/model_\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file ./output_dir/model_\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"./output_dir/model_\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ./output_dir/model_\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./output_dir/model_.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline('summarization',                                                # pipeline de résumé automatique \n",
    "                        model=output_dir + '/model_',                                 # chemin du modèle\n",
    "                        tokenizer=tokenizer,                                          # tokenizer\n",
    "                        framework='pt')                                               # framework utilisé (pytorch) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style='color:red;'>Notice</p> \n",
    "#### Il y a une grosse limite ici, l'entraînement se fait sur un seul GPU, donc ça prend beaucoup de temps pour entraîner le modèle, j'ai essayé de l'entraîner sur un Google Colab Pro, mais c'était encore trop lent, j'ai donc décidé d'utiliser wandb pour suivre le processus d'entraînement et l'arrêter quand je le souhaite, puis j'enregistre le modèle et l'utilise pour générer des résumés.\n",
    "\n",
    "#### Même si j'ai entraîné le modèle pendant 1 époque, il m'a fallu plus de 6 heures pour l'entraîner, j'ai donc décidé d'utiliser un modèle pré-entraîné, que je vous montrerai dans la section suivante.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de résumés à l'aide du modèle t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):                                                              # fonction pour générer le résumé\n",
    "\n",
    "    model = AutoModelWithLMHead.from_pretrained(\"t5-small\")                              # charger le modèle\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")                                # charger le tokenizer\n",
    "\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text,                                      # encode le texte à résumer\n",
    "                                return_tensors=\"pt\",                                     # type de tenseur de sortie (ici pytorch)\n",
    "                                max_length=512,                                          # taille maximale de l'entrée (512 pour T5)\n",
    "                                truncation=True)                                         # tronquer le texte si sa taille est supérieure à la taille maximale de l'entrée\n",
    "\n",
    "    outputs = model.generate(inputs,                                                     # générer le résumé\n",
    "                                max_length=250,                                          # taille maximale de la sortie (ici 250) \n",
    "                                min_length=80,                                           # taille minimale de la sortie (ici 80)\n",
    "                                length_penalty=2.0,                                      # pénalité de longueur (plus elle est élevée, plus le modèle génère un résumé plus long)\n",
    "                                num_beams=4,                                             # numéro de faisceaux (plus il est élevé, plus le modèle génère un résumé plus long) \n",
    "                                early_stopping=True)                                     # si on veut arrêter l'entraînement quand on atteint la taille maximale de la sortie \n",
    "    return tokenizer.decode(outputs[0])                                                  # retourner le résumé\n",
    "\n",
    "\n",
    "def generate_summary_for_text(text):                                                     # fonction pour générer le résumé pour un texte donné\n",
    "\n",
    "    # text = re.sub(r'\\d+', '', text)                                                    # supprimer les chiffres du texte (si on veut)\n",
    "    text = re.sub(' +', ' ', text)                                                       # supprimer les espaces multiples\n",
    "    text = re.sub('\\n+', ' ', text)                                                      # supprimer les sauts de ligne multiples\n",
    "    text = re.sub('\\t+', ' ', text)                                                      # supprimer les tabulations multiples\n",
    "\n",
    "    summary = generate_summary(text)                                                     # générer le résumé du texte             \n",
    "    return summary                                                                       # retourner le résumé généré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted summary :\n",
      " <pad> a common technical definition of a recession is two successive quarters of negative growth. on an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. japan's economy teetered on the brink of a technical recession in the three months to September, figures show.</s>\n",
      " \n",
      "Original summary :\n",
      " Japan narrowly escapes recession\n",
      "\n",
      "Japan's economy teetered on the brink of a technical recession in the three months to September, figures show.\n",
      "\n",
      "Revised figures indicated growth of just 0.1% - and a similar-sized contraction in the previous quarter. On an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. A common technical definition of a recession is two successive quarters of negative growth.\n",
      "\n",
      "The government was keen to play down the worrying implications of the data. \"I maintain the view that Japan's economy remains in a minor adjustment phase in an upward climb, and we will monitor developments carefully,\" said economy minister Heizo Takenaka. But in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead, observers were less sanguine. \"It's painting a picture of a recovery... much patchier than previously thought,\" said Paul Sheard, economist at Lehman Brothers in Tokyo. Improvements in the job market apparently have yet to feed through to domestic demand, with private consumption up just 0.2% in the third quarter.\n",
      "\n",
      "Predicted summary word count :  56\n",
      "Original summary word count :  184\n"
     ]
    }
   ],
   "source": [
    "summary_ = generate_summary_for_text(df['original'][5])                                  # générer le résumé pour le texte d'index 5\n",
    "\n",
    "print('Predicted summary :\\n',summary_)\n",
    "print(\" \")\n",
    "print('Original summary :\\n',df['summary'][5])\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "print('Predicted summary word count : ',count_words(summary_))\n",
    "print('Original summary word count : ',count_words(df['summary'][5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a63501da4ff5345f792f8d85fdbe5e308a00ec1c83390f9577f8c8a1adb4af3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
